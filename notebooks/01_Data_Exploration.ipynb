{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc83119-3dfd-4b69-ba9a-8ef7420e827a",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Exploration (for Edge-IIoT)\n",
    "\n",
    "**Goal:** Establish a factual understanding of the raw dataset that will feed the FL-IDS for IIoT (surveillance). This notebook inspects structure, label balance, and data quality. It does not modify or export data.\n",
    "\n",
    "**Objectives**\n",
    "- Load the raw CSV from `data/raw/...`.\n",
    "- Inspect shape, columns, dtypes, and memory footprint.\n",
    "- Check class balance for `Attack_label` and enumerate `Attack_type`.\n",
    "- Identify potential data quality issues (missingness, mixed types, high-cardinality text).\n",
    "- List actions to perform later in data preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a04ec9-1f52-4b5e-a646-42ee5be66778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2284b2-4021-4bdf-b225-a03d1ddd997f",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Structure\n",
    "\n",
    "This section confirms that the expected file is present and examines high-level structure:\n",
    "\n",
    "- File path used in this run (printed above).\n",
    "- Dataset shape (rows Ã— columns).\n",
    "- A plaintext list of all columns to anchor later selections.\n",
    "- Memory footprint will be captured via `df.info()` below.\n",
    "\n",
    "**Notes**\n",
    "- Keep `low_memory=False` to avoid misleading dtypes on wide CSVs.\n",
    "- If the file is missing, place it under `data/raw/` and re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816d2149-9b2d-42fd-864b-e5f36de84cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset: (2219201, 63)\n",
      "The columns of the dataset are: \n",
      " ['frame.time', 'ip.src_host', 'ip.dst_host', 'arp.dst.proto_ipv4', 'arp.opcode', 'arp.hw.size', 'arp.src.proto_ipv4', 'icmp.checksum', 'icmp.seq_le', 'icmp.transmit_timestamp', 'icmp.unused', 'http.file_data', 'http.content_length', 'http.request.uri.query', 'http.request.method', 'http.referer', 'http.request.full_uri', 'http.request.version', 'http.response', 'http.tls_port', 'tcp.ack', 'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin', 'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack', 'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.options', 'tcp.payload', 'tcp.seq', 'tcp.srcport', 'udp.port', 'udp.stream', 'udp.time_delta', 'dns.qry.name', 'dns.qry.name.len', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission', 'dns.retransmit_request', 'dns.retransmit_request_in', 'mqtt.conack.flags', 'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len', 'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.msgtype', 'mqtt.proto_len', 'mqtt.protoname', 'mqtt.topic', 'mqtt.topic_len', 'mqtt.ver', 'mbtcp.len', 'mbtcp.trans_id', 'mbtcp.unit_id', 'Attack_label', 'Attack_type']\n"
     ]
    }
   ],
   "source": [
    "#Defining the paths and loading the dataset \n",
    "file_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\raw\\archive\\DNN-EdgeIIoT-dataset.csv\"\n",
    "df = pd.read_csv(file_path, low_memory = False)\n",
    "\n",
    "#Display Preliminary info about the dataset\n",
    "print (f\"The shape of the dataset: {df.shape}\")\n",
    "print (\"The columns of the dataset are: \\n\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19464c5d-1f6f-4186-8eaf-ed5b2c246f0b",
   "metadata": {},
   "source": [
    "## 2. Class Distribution and Attack Types\n",
    "\n",
    "We summarise label balance for:\n",
    "- `Attack_label` (binary target: 0 = normal, 1 = attack).\n",
    "- `Attack_type` (multi-class taxonomy of specific attacks).\n",
    "\n",
    "**Why this matters**\n",
    "- Guides metrics and sampling strategies.\n",
    "- Informs whether we need stratified splits and per-class reporting.\n",
    "\n",
    "Record both counts and percentages, and note any rare classes that may affect evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db664715-2b1e-4bb2-b041-9d89db0c080d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Attack_type\n",
       " Normal                   1615643\n",
       " DDoS_UDP                  121568\n",
       " DDoS_ICMP                 116436\n",
       " SQL_injection              51203\n",
       " Password                   50153\n",
       " Vulnerability_scanner      50110\n",
       " DDoS_TCP                   50062\n",
       " DDoS_HTTP                  49911\n",
       " Uploading                  37634\n",
       " Backdoor                   24862\n",
       " Port_Scanning              22564\n",
       " XSS                        15915\n",
       " Ransomware                 10925\n",
       " MITM                        1214\n",
       " Fingerprinting              1001\n",
       " Name: count, dtype: int64,\n",
       " Attack_label\n",
       " 0    1615643\n",
       " 1     603558\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the unique values in \"Attack_type\" & \"Attack_label\" to see the type of the attacks this dataset cover\n",
    "attack_types  = df[\"Attack_type\"].value_counts()\n",
    "attack_labels = df[\"Attack_label\"].value_counts()\n",
    "\n",
    "attack_types , attack_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48d3b7-e00c-4988-bef2-039dd37b2e9d",
   "metadata": {},
   "source": [
    "## 3. Data Types, Missingness, and Potential Quality Risks\n",
    "\n",
    "We capture:\n",
    "- Full dtype table from `df.info()`.\n",
    "- Column-wise missing value counts.\n",
    "- Early flags for columns with mixed or ambiguous types.\n",
    "\n",
    "**Interpretation checklist**\n",
    "- Columns expected to be numeric but parsed as `object` (e.g., ports, lengths).\n",
    "- High-cardinality text fields (IPs, URIs, topics) that are unsuitable for standard models without encoding or pruning.\n",
    "- Columns that may be redundant or leak labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be348d72-c652-468d-bb7d-2aa3ad9f4494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2219201 entries, 0 to 2219200\n",
      "Data columns (total 63 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   frame.time                 object \n",
      " 1   ip.src_host                object \n",
      " 2   ip.dst_host                object \n",
      " 3   arp.dst.proto_ipv4         object \n",
      " 4   arp.opcode                 float64\n",
      " 5   arp.hw.size                float64\n",
      " 6   arp.src.proto_ipv4         object \n",
      " 7   icmp.checksum              float64\n",
      " 8   icmp.seq_le                float64\n",
      " 9   icmp.transmit_timestamp    float64\n",
      " 10  icmp.unused                float64\n",
      " 11  http.file_data             object \n",
      " 12  http.content_length        float64\n",
      " 13  http.request.uri.query     object \n",
      " 14  http.request.method        object \n",
      " 15  http.referer               object \n",
      " 16  http.request.full_uri      object \n",
      " 17  http.request.version       object \n",
      " 18  http.response              float64\n",
      " 19  http.tls_port              float64\n",
      " 20  tcp.ack                    float64\n",
      " 21  tcp.ack_raw                float64\n",
      " 22  tcp.checksum               float64\n",
      " 23  tcp.connection.fin         float64\n",
      " 24  tcp.connection.rst         float64\n",
      " 25  tcp.connection.syn         float64\n",
      " 26  tcp.connection.synack      float64\n",
      " 27  tcp.dstport                float64\n",
      " 28  tcp.flags                  float64\n",
      " 29  tcp.flags.ack              float64\n",
      " 30  tcp.len                    float64\n",
      " 31  tcp.options                object \n",
      " 32  tcp.payload                object \n",
      " 33  tcp.seq                    float64\n",
      " 34  tcp.srcport                object \n",
      " 35  udp.port                   float64\n",
      " 36  udp.stream                 float64\n",
      " 37  udp.time_delta             float64\n",
      " 38  dns.qry.name               float64\n",
      " 39  dns.qry.name.len           object \n",
      " 40  dns.qry.qu                 float64\n",
      " 41  dns.qry.type               float64\n",
      " 42  dns.retransmission         float64\n",
      " 43  dns.retransmit_request     float64\n",
      " 44  dns.retransmit_request_in  float64\n",
      " 45  mqtt.conack.flags          object \n",
      " 46  mqtt.conflag.cleansess     float64\n",
      " 47  mqtt.conflags              float64\n",
      " 48  mqtt.hdrflags              float64\n",
      " 49  mqtt.len                   float64\n",
      " 50  mqtt.msg_decoded_as        float64\n",
      " 51  mqtt.msg                   object \n",
      " 52  mqtt.msgtype               float64\n",
      " 53  mqtt.proto_len             float64\n",
      " 54  mqtt.protoname             object \n",
      " 55  mqtt.topic                 object \n",
      " 56  mqtt.topic_len             float64\n",
      " 57  mqtt.ver                   float64\n",
      " 58  mbtcp.len                  float64\n",
      " 59  mbtcp.trans_id             float64\n",
      " 60  mbtcp.unit_id              float64\n",
      " 61  Attack_label               int64  \n",
      " 62  Attack_type                object \n",
      "dtypes: float64(42), int64(1), object(20)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "#Checking the data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a48651c-89ea-4e09-b218-06a9bb827a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cOUNT of missing values for each column in the dataset\n",
    "missing = df.isnull().sum()\n",
    "missing[missing > 0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5cd39-2f91-464e-81ad-73706235b69f",
   "metadata": {},
   "source": [
    "## 4. Categorical and Protocol Fields â€” Sanity Checks\n",
    "\n",
    "We inspect uniqueness samples for selected fields:\n",
    "- `http.request.method`, `http.request.version`\n",
    "- `tcp.srcport`\n",
    "- `dns.qry.name.len`\n",
    "\n",
    "**What to look for**\n",
    "- Unexpected tokens embedded in protocol/version fields (e.g., payload fragments or injections).\n",
    "- Numeric-looking fields stored as strings.\n",
    "- Parsing artefacts that will require cleaning later.\n",
    "\n",
    "No transformations are performed here; this is an audit to inform preparation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fad831b-2a47-49d2-9ac0-c71cbe76a80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Http.request.method: ['0.0' '0' 'GET' 'POST' 'PROPFIND' 'TRACE' 'OPTIONS' 'PUT' 'SEARCH']\n",
      "Http.request.version : ['0.0' '0' 'HTTP/1.1' 'HTTP/1.0'\n",
      " \"name=a><input name=i value=XSS>&lt;script>alert('Vulnerable')</script> HTTP/1.1\"\n",
      " 'Src=javascript:alert(\\'Vulnerable\\')><Img Src=\\\\\" HTTP/1.1' '> HTTP/1.1'\n",
      " 'script>alert(1)/script><\\\\\" HTTP/1.1'\n",
      " '-al&_PHPLIB[libdir]=http://cirt.net/rfiinc.txt?? HTTP/1.1' '-a HTTP/1.1'\n",
      " '/etc/passwd|?data=Download HTTP/1.1'\n",
      " '-al&ABSOLUTE_PATH_STUDIP=http://cirt.net/rfiinc.txt?? HTTP/1.1'\n",
      " 'By Dr HTTP/1.1']\n",
      "tcp.srcport sample: ['1883.0' '64855.0' '64856.0' '64857.0' '64858.0' '64859.0' '64860.0'\n",
      " '64861.0' '64862.0' '64863.0']\n",
      "dns.qry.name.len sample: ['0' '0.debian.pool.ntp.org' '1.debian.pool.ntp.org'\n",
      " '2.debian.pool.ntp.org' '3.debian.pool.ntp.org' '_googlecast._tcp.local'\n",
      " 'raspberrypi.local' 'null-null.local' '0.0' '1.0']\n"
     ]
    }
   ],
   "source": [
    "#Check the unique values of potential categorical fiedls such as the http request method and version\n",
    "unique_http_method = df[\"http.request.method\"].unique()\n",
    "unique_http_version = df[\"http.request.version\"].unique()\n",
    "unique_tcpport = df[\"tcp.srcport\"].unique()[:10] #first ten should be enough to get an idea if it is numeric\n",
    "unique_dns_namelen = df[\"dns.qry.name.len\"].unique()[:10] #first ten should be enough to get an idea if it is numeric\n",
    "\n",
    "print(f\"Http.request.method: {unique_http_method}\")\n",
    "print(f\"Http.request.version : {unique_http_version}\")\n",
    "print(f\"tcp.srcport sample: {unique_tcpport}\")\n",
    "print(f\"dns.qry.name.len sample: {unique_dns_namelen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32dd42f-1655-449a-b309-3b8b20f3ca44",
   "metadata": {},
   "source": [
    "## 5. Notes and Next Steps\n",
    "\n",
    "**Key EDA findings**\n",
    "- Summarise any missing values, suspicious dtypes, and extreme cardinality fields identified above.\n",
    "- List columns that look text-heavy or noisy and likely to be dropped or transformed later.\n",
    "\n",
    "**Next steps**\n",
    "- Column selection and safe casting plan.\n",
    "- Handling of high-cardinality text (e.g., IPs, URIs, MQTT topics).\n",
    "- Definitions of train/test sets for supervised and unsupervised pipelines.\n",
    "- Reproducibility choices (random seeds) and consistent, repo-relative paths.\n",
    "\n",
    "The actual cleaning, splitting, and exports are performed in the subsequent section(s) to keep this EDA record clear and auditable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898ce76-78af-4b77-a4b8-a5332c768c3c",
   "metadata": {},
   "source": [
    "## 6. Cleaning the Dataset for Binary Classification\n",
    "\n",
    "This step standardises the raw Edge-IIoT data for downstream supervised and unsupervised pipelinesâ€”without changing labels.\n",
    "\n",
    "**Rationale**\n",
    "- Drop high-cardinality / unstructured fields (IPs, URIs, raw payloads, free-text) that are not robust for our models.\n",
    "- Ensure key network fields are in numeric form (e.g., `tcp.srcport`).\n",
    "- Use `Attack_label` as the binary target and remove `Attack_type` (multi-class taxonomy not used in binary models).\n",
    "\n",
    "**Operations performed**\n",
    "- Drop columns: IPs/URIs/payloads and related free-text protocol fields (see code list below for exact names).\n",
    "- Convert `tcp.srcport` to numeric (coercing errors), drop rows where it is invalid/NaN, then cast to integer.\n",
    "- Retain only modeling-relevant features + `Attack_label`.\n",
    "\n",
    "**Outputs**\n",
    "- Cleaned CSV for supervised work.\n",
    "- Identical cleaned CSV for unsupervised work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545600ea-abd1-4c76-b973-d6f2fdffb51f",
   "metadata": {},
   "source": [
    "### Cleaned Data Artifacts (for later use)\n",
    "\n",
    "Saved cleaned datasets for both tracks:\n",
    "\n",
    "- **Supervised:** `data/processed/surv_supervised/surv_cleaned_supervised.csv`\n",
    "- **Unsupervised:** `data/processed/surv_unsupervised/surv_cleaned_unsupervised.csv`\n",
    "\n",
    "**Note**\n",
    "- Keep these paths repo-relative in code where possible.\n",
    "- These files are large and should remain git-ignored (`data/**`), but paths are documented here for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fdcdcbc-9d78-4fd8-ada1-68c227059a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in tcp.srcport: 367\n",
      "The Cleaned dataset shape after dropping the invalid tcp.srcport rows: (2218834, 43)\n",
      "Cleaned dataset saved to:\n",
      "Supervised: D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_supervised\\surv_cleaned_supervised.csv\n",
      "Unsupervised: D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_unsupervised\\surv_cleaned_unsupervised.csv\n"
     ]
    }
   ],
   "source": [
    "#Columns to drop\n",
    "columns_to_drop = [\n",
    "    \"ip.src_host\", \"ip.dst_host\", \"arp.dst.proto_ipv4\", \"arp.src.proto_ipv4\",\"http.file_data\", \"http.request.uri.query\",\n",
    "    \"http.request.full_uri\", \"http.referer\",\"dns.qry.name\", \"dns.qry.name.len\", \"mqtt.topic\", \"mqtt.msg\", \n",
    "    \"mqtt.msg_decoded_as\",\"mqtt.protoname\", \"tcp.options\", \"tcp.payload\", \"mqtt.conack.flags\",\n",
    "    \"http.request.version\", \"frame.time\", \"Attack_type\"\n",
    "]\n",
    "\n",
    "#Drop the columns\n",
    "df_cleaned = df.drop(columns = columns_to_drop)\n",
    "\n",
    "# Convert tcp.srcport to numeric, coerce errors to NaNs\n",
    "df_cleaned['tcp.srcport'] = pd.to_numeric(df_cleaned['tcp.srcport'], errors='coerce')\n",
    "\n",
    "#Double Checking:\n",
    "print(f\"NaNs in tcp.srcport: {df_cleaned['tcp.srcport'].isna().sum()}\")\n",
    "\n",
    "# Drop rows where tcp.srcport is NaN\n",
    "df_cleaned.dropna(subset=['tcp.srcport'], inplace=True)\n",
    "# Convert to integer now that NaNs are gone\n",
    "df_cleaned['tcp.srcport'] = df_cleaned['tcp.srcport'].astype(int)\n",
    "\n",
    "print(f\"The Cleaned dataset shape after dropping the invalid tcp.srcport rows: {df_cleaned.shape}\")\n",
    "\n",
    "# Saving the cleaned dataset to both supervised and unsupervised folders\n",
    "output_supervised = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_supervised\\surv_cleaned_supervised.csv\"\n",
    "output_unsupervised = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_unsupervised\\surv_cleaned_unsupervised.csv\"\n",
    "\n",
    "#Save the dataset (same file for both for now)\n",
    "df_cleaned.to_csv(output_supervised, index=False)\n",
    "df_cleaned.to_csv(output_unsupervised, index=False)\n",
    "print(\"Cleaned dataset saved to:\")\n",
    "print(f\"Supervised: {output_supervised}\")\n",
    "print(f\"Unsupervised: {output_unsupervised}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0d2b8-e3ba-4631-904f-7939d04d3cff",
   "metadata": {},
   "source": [
    "## 8. Supervised 80/20 Split (with and without SMOTE)\n",
    "\n",
    "We create two supervised training variants from the cleaned dataset:\n",
    "\n",
    "1) **No-SMOTE:** baseline stratified 80/20 split.  \n",
    "2) **With-SMOTE:** same split, then apply SMOTE **on the training set only** to reduce class imbalance.\n",
    "\n",
    "**Procedure**\n",
    "- Read `surv_cleaned_supervised.csv`.\n",
    "- Separate features/labels (`Attack_label`).\n",
    "- Perform **stratified** `train_test_split` (80/20, `random_state=42`).\n",
    "- If present, label-encode `http.request.method` (a small categorical field) to maintain numeric design.\n",
    "- Save both variants for later use.\n",
    "\n",
    "**Saved outputs**\n",
    "- `data/processed/surv_supervised/80_20/no_smote/train.csv`  \n",
    "- `data/processed/surv_supervised/80_20/no_smote/test.csv`  \n",
    "- `data/processed/surv_supervised/80_20/with_smote/train.csv`  \n",
    "- `data/processed/surv_supervised/80_20/with_smote/test.csv`  *(test unchanged)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b69e2d3-b9dc-485e-8185-3f028eaee4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 80/20 split without SMOTE.\n",
      "Saved 80/20 split with SMOTE applied to training set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cleaned_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_supervised\\surv_cleaned_supervised.csv\"\n",
    "df_cleaned = pd.read_csv(cleaned_path, low_memory = False)\n",
    "\n",
    "#Seperate the features from labels\n",
    "X = df_cleaned.drop(columns=['Attack_label'])\n",
    "y = df_cleaned['Attack_label']\n",
    "\n",
    "# 80/20 Split Stratified Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Label encode 'http.request.method' if it exists\n",
    "if 'http.request.method' in X_train.columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train['http.request.method'] = le.fit_transform(X_train['http.request.method'])\n",
    "    X_test['http.request.method'] = le.transform(X_test['http.request.method'])\n",
    "\n",
    "# Save non-SMOTE version\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "no_smote_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_supervised\\80_20\\no_smote\"\n",
    "\n",
    "os.makedirs(no_smote_path, exist_ok=True)\n",
    "train_df.to_csv(os.path.join(no_smote_path, \"train.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(no_smote_path, \"test.csv\"), index=False)\n",
    "print(\"Saved 80/20 split without SMOTE.\")\n",
    "\n",
    "# === SMOTE version ===\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "train_smote_df = pd.concat(\n",
    "    [pd.DataFrame(X_train_sm, columns=X_train.columns),\n",
    "      pd.Series(y_train_sm, name='Attack_label')], axis=1\n",
    " )\n",
    "\n",
    "smote_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_supervised\\80_20\\with_smote\"\n",
    "os.makedirs(smote_path, exist_ok=True)\n",
    "train_smote_df.to_csv(os.path.join(smote_path, \"train.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(smote_path, \"test.csv\"), index=False) # unchanged\n",
    "print(\"Saved 80/20 split with SMOTE applied to training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e776dc-e481-4afa-bbfc-6b8422e49c06",
   "metadata": {},
   "source": [
    "## 9. Preparing Unsupervised Anomaly-Detection Sets\n",
    "\n",
    "Unsupervised models (Isolation Forest, Autoencoder) learn only from **normal** traffic and are evaluated on mixed traffic.\n",
    "\n",
    "**Procedure**\n",
    "- Read `surv_cleaned_unsupervised.csv`.\n",
    "- **Training set:** all rows where `Attack_label == 0` (normal-only).\n",
    "- **Testing set:** full cleaned dataset (normal + attack).\n",
    "- Save both artifacts for downstream unsupervised modeling.\n",
    "\n",
    "**Saved outputs**\n",
    "- `data/processed/surv_unsupervised/train_normal_only.csv`\n",
    "- `data/processed/surv_unsupervised/test_mixed.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfdcf689-b972-4da6-811e-1e688f7328ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised dataset prepared and saved:\n",
      "- Training set (normal only): (1615643, 43)\n",
      "- Testing set (mixed): (2218834, 43)\n"
     ]
    }
   ],
   "source": [
    "# Load the already-cleaned dataset\n",
    "cleaned_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_unsupervised\\surv_cleaned_unsupervised.csv\"\n",
    "df_cleaned = pd.read_csv(cleaned_path, low_memory = False)\n",
    "\n",
    "# Training set: only normal data (Attack_label == 0)\n",
    "df_train_unsup = df_cleaned[df_cleaned['Attack_label'] == 0]\n",
    "\n",
    "# Testing set: full data (mixed normal + attack)\n",
    "df_test_unsup = df_cleaned.copy()\n",
    "\n",
    "# Save both files\n",
    "output_path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_unsupervised\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "df_train_unsup.to_csv(os.path.join(output_path, \"train_normal_only.csv\"), index = False)\n",
    "df_test_unsup.to_csv(os.path.join(output_path, \"test_mixed.csv\"), index = False)\n",
    " \n",
    "print(\"Unsupervised dataset prepared and saved:\")\n",
    "print(f\"- Training set (normal only): {df_train_unsup.shape}\")\n",
    "print(f\"- Testing set (mixed): {df_test_unsup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56dfee2-4262-4d0f-92a8-437f136fcb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeedba0-5b41-40d1-b42c-49ff0fb78bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1615643, 43)\n",
      "Missing values before cleaning:\n",
      " arp.opcode                 0\n",
      "arp.hw.size                0\n",
      "icmp.checksum              0\n",
      "icmp.seq_le                0\n",
      "icmp.transmit_timestamp    0\n",
      "dtype: int64\n",
      "Shape after dropping NaNs: (1615643, 42)\n",
      "Client 1: 323128 rows saved to D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\federated\\unsupervised\\client_1/train.csv\n",
      "Client 2: 323128 rows saved to D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\federated\\unsupervised\\client_2/train.csv\n",
      "Client 3: 323128 rows saved to D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\federated\\unsupervised\\client_3/train.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Load the full normal-only dataset\n",
    "path = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\surv_unsupervised\\train_normal_only.csv\"\n",
    "df = pd.read_csv(path)\n",
    "print(\"Original shape:\", df.shape)\n",
    "\n",
    "#Dropping the problematic columns\n",
    "problem_cols = ['http.request.method']\n",
    "df.drop(columns=[col for col in problem_cols if col in df.columns], inplace=True)\n",
    "\n",
    "#Check the NaNs\n",
    "print(\"Missing values before cleaning:\\n\", df.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "# Drop rows with any NaNs\n",
    "df.dropna(inplace=True)\n",
    "print(\"Shape after dropping NaNs:\", df.shape)\n",
    "\n",
    "#Shuffle the dataset before the split\n",
    "df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Split into 5 equal parts\n",
    "n_clients = 5\n",
    "chunk_size = len(df) // n_clients\n",
    "\n",
    "output_dir = r\"D:\\August-Thesis\\FL-IDS-Surveillance\\data\\processed\\federated\\unsupervised\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(n_clients):\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < n_clients - 1 else len(df)\n",
    "    df_client = df.iloc[start:end].copy()\n",
    "    client_path = os.path.join(output_dir, f\"client_{i+1}\")\n",
    "    os.makedirs(client_path, exist_ok=True)\n",
    "    df_client.to_csv(os.path.join(client_path, \"train.csv\"), index=False)\n",
    "    print(f\"Client {i+1}: {df_client.shape[0]} rows saved to {client_path}/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323724f-72d4-4cce-a879-101b9b692517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AR-iForest)",
   "language": "python",
   "name": "ar_iforest_fl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
