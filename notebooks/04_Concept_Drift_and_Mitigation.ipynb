{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4698b7a-7347-4ad4-a3dd-9101a19d2985",
   "metadata": {},
   "source": [
    "# 04: Concept Drift Detection & Mitigation\n",
    "\n",
    "This notebook implements a full pipeline for detecting, analyzing, and mitigating concept drift in Federated Learning for Industrial IoT (IIoT) anomaly detection using Autoencoders.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook Objectives**\n",
    "\n",
    "1. **Detect Drift**  \n",
    "   - Use statistical methods: KS-test + Bonferroni correction and Population Stability Index (PSI)\n",
    "   - Identify which features have drifted and how severely\n",
    "\n",
    "2. **Classify Drift**  \n",
    "   - Use feature variance to distinguish:\n",
    "     - **Healthy Drift** (low-impact features, low variance)\n",
    "     - **Unhealthy Drift** (high-variance or security-relevant features)\n",
    "\n",
    "3. **Simulate Drift Types**\n",
    "   - Gradual Drift: Noise + systematic shift over time\n",
    "   - Sudden Drift: Abrupt scaling at a midpoint\n",
    "   - Healthy Drift: Subtle changes on low-impact features\n",
    "\n",
    "4. **Run Federated Learning with Drift Injection**\n",
    "   - Inject drift in Client 3 from Round 6 onward\n",
    "   - Track detection and model performance over 20 FL rounds\n",
    "\n",
    "5. **Mitigate Unhealthy Drift**\n",
    "   - **Local Retraining**: Client 3 adapts locally\n",
    "   - **Model Reset**: Revert to latest global model\n",
    "   - **Client Freezing**: Temporarily exclude from aggregation\n",
    "\n",
    "6. **Adaptive Aggregation**\n",
    "   - Apply Drift-Aware Weighting based on KS-test drift score\n",
    "   - Adjust client contribution dynamically each round\n",
    "\n",
    "7. **Stress-Test Limits**\n",
    "   - Test resilience under extreme drift conditions or multi-client drift\n",
    "   - Evaluate when strategies break or fail\n",
    "\n",
    "8. **Summarize Findings**\n",
    "   - Compare all mitigation strategies across metrics (F1, Recall, FP/FN)\n",
    "   - Identify which approaches are best for which drift scenarios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530a6969-1a4d-4503-976f-d0f0a2ed8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean client data loaded and scaled.\n",
      "Saved MinMaxScaler to: D:/August-Thesis/FL-IDS-Surveillance/notebooks/results/scalers\\minmax_scaler_client_3.pkl\n",
      "Saved StandardScaler to: D:/August-Thesis/FL-IDS-Surveillance/notebooks/results/scalers\\standard_scaler_client_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# === Part 1: Setup and Preprocessing ===\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# === Paths ===\n",
    "client_id = \"client_3\"\n",
    "client_data_path = f\"D:/August-Thesis/FL-IDS-Surveillance/data/processed/federated/unsupervised/{client_id}/train.csv\"\n",
    "\n",
    "# Output directory for the two scalers\n",
    "scaler_dir = \"D:/August-Thesis/FL-IDS-Surveillance/notebooks/results/scalers\"\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "\n",
    "# === Load client training data ===\n",
    "df_clean = pd.read_csv(client_data_path, low_memory=False)\n",
    "\n",
    "# Drop label and irrelevant columns\n",
    "X_clean = df_clean.drop(columns=[\"Attack_label\", \"http.request.method\"], errors=\"ignore\")\n",
    "X_clean = X_clean.select_dtypes(include=\"number\")\n",
    "\n",
    "feature_names = X_clean.columns.tolist()\n",
    "\n",
    "# === Fit the MinMaxScaler for AE training ===\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_clean_minmax = minmax_scaler.fit_transform(X_clean)\n",
    "\n",
    "# === Fit StandardScaler for statistical drift detection ===\n",
    "standard_scaler = StandardScaler()\n",
    "X_clean_standard = standard_scaler.fit_transform(X_clean)\n",
    "\n",
    "# === save the  scalers ===\n",
    "minmax_path = os.path.join(scaler_dir, f\"minmax_scaler_{client_id}.pkl\")\n",
    "standard_path = os.path.join(scaler_dir, f\"standard_scaler_{client_id}.pkl\")\n",
    "\n",
    "joblib.dump(minmax_scaler, minmax_path)\n",
    "joblib.dump(standard_scaler, standard_path)\n",
    "\n",
    "print(\"Clean client data loaded and scaled.\")\n",
    "print(f\"Saved MinMaxScaler to: {minmax_path}\")\n",
    "print(f\"Saved StandardScaler to: {standard_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589efcf7-3aa5-4fe7-a608-8420f1c75ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Part 2: Drift Detection Utilities ===\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def ks_psi_drift_detection(X_ref, X_new, feature_names, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Detect drift using KS-test (with Bonferroni correction) and PSI per feature.\n",
    "    both X_ref and X_new must be standardized using StandardScaler.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # KS-Test (with Bonferroni correction)\n",
    "    ks_pvalues = []\n",
    "    for i in range(X_ref.shape[1]):\n",
    "        ks_stat, p_value = stats.ks_2samp(X_ref[:, i], X_new[:, i])\n",
    "        ks_pvalues.append(p_value)\n",
    "\n",
    "    corrected_alpha = significance_level / len(ks_pvalues)\n",
    "    drifted_ks_features = [feature_names[i] for i, p in enumerate(ks_pvalues) if p < corrected_alpha]\n",
    "\n",
    "    # The PSI Calculation \n",
    "    def compute_psi(expected, actual, buckets=10):\n",
    "        def scale_range(arr):\n",
    "            arr = np.array(arr)\n",
    "            return (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "\n",
    "        expected_scaled = scale_range(expected)\n",
    "        actual_scaled = scale_range(actual)\n",
    "        breakpoints = np.linspace(0, 1, buckets + 1)\n",
    "        expected_counts = np.histogram(expected_scaled, breakpoints)[0]\n",
    "        actual_counts = np.histogram(actual_scaled, breakpoints)[0]\n",
    "\n",
    "        expected_percents = expected_counts / len(expected_scaled)\n",
    "        actual_percents = actual_counts / len(actual_scaled)\n",
    "\n",
    "        expected_percents = np.where(expected_percents == 0, 1e-6, expected_percents)\n",
    "        actual_percents = np.where(actual_percents == 0, 1e-6, actual_percents)\n",
    "\n",
    "        psi = np.sum((actual_percents - expected_percents) * np.log(actual_percents / expected_percents))\n",
    "        return psi\n",
    "\n",
    "    psi_scores = [compute_psi(X_ref[:, i], X_new[:, i]) for i in range(X_ref.shape[1])]\n",
    "    drifted_psi_features = [feature_names[i] for i, score in enumerate(psi_scores) if score > 0.2]\n",
    "\n",
    "    # Final Results \n",
    "    results['ks_drifted_features'] = drifted_ks_features\n",
    "    results['ks_pvalues'] = ks_pvalues\n",
    "    results['psi_drifted_features'] = drifted_psi_features\n",
    "    results['psi_scores'] = psi_scores\n",
    "    results['ks_drift'] = len(drifted_ks_features) > 0\n",
    "    results['psi_drift'] = len(drifted_psi_features) > 0\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95b76e1-6903-4c5b-8113-c071425f29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Part 3: Drift Simulation functions ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_gradual_drift(df, features, noise_factor=0.2, shift_factor=0.3):\n",
    "    df_drifted = df.copy()\n",
    "    n_rows = len(df)\n",
    "    time_idx = np.linspace(0, 1, n_rows)\n",
    "\n",
    "    for feature in features:\n",
    "        if feature in df_drifted.columns:\n",
    "            std = df[feature].std()\n",
    "            shift = shift_factor * std * time_idx\n",
    "            noise = np.random.normal(0, noise_factor * std * time_idx)\n",
    "            df_drifted[feature] = df_drifted[feature].astype(float) + shift + noise\n",
    "\n",
    "    return df_drifted\n",
    "\n",
    "\n",
    "def simulate_sudden_drift(df, features, sudden_factor=1.5, drift_point=0.5):\n",
    "    df_drifted = df.copy()\n",
    "    n_rows = len(df)\n",
    "    drift_idx = int(drift_point * n_rows)\n",
    "\n",
    "    for feature in features:\n",
    "        if feature in df_drifted.columns:\n",
    "            df_drifted[feature] = df_drifted[feature].astype(float)\n",
    "            df_drifted.loc[drift_idx:, feature] *= sudden_factor\n",
    "\n",
    "    return df_drifted\n",
    "\n",
    "\n",
    "def simulate_healthy_drift(df, features, noise_factor=0.02, shift_factor=0.01):\n",
    "    df_drifted = df.copy()\n",
    "    n_rows = len(df)\n",
    "    time_idx = np.linspace(0, 1, n_rows)\n",
    "\n",
    "    for feature in features:\n",
    "        if feature in df_drifted.columns:\n",
    "            std = df[feature].std()\n",
    "            shift = shift_factor * std * time_idx\n",
    "            noise = np.random.normal(0, noise_factor * std * time_idx)\n",
    "            df_drifted[feature] = df_drifted[feature].astype(float) + shift + noise\n",
    "\n",
    "    return df_drifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0d214d-bf97-4e8e-9610-50c04f262adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drifted datasets saved:\n",
      "  Gradual Drift: ['tcp.ack_raw', 'tcp.seq', 'tcp.ack', 'tcp.dstport', 'tcp.srcport']\n",
      "  Sudden Drift: ['tcp.ack_raw', 'tcp.seq', 'tcp.ack', 'tcp.dstport', 'tcp.srcport']\n",
      "  Healthy Drift: ['udp.stream', 'udp.port', 'tcp.checksum']\n"
     ]
    }
   ],
   "source": [
    "# === Select Features for Drift Simulation ===\n",
    "\n",
    "# Top 5 high-variance features => Unhealthy Drift\n",
    "top5_drift_features = X_clean.var().sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "# Low-impact 3 features => Healthy Drift\n",
    "healthy_features = X_clean.var().sort_values().tail(8).head(3).index.tolist()\n",
    "\n",
    "# === Simulate Drifted Datasets ===\n",
    "\n",
    "df_gradual = simulate_gradual_drift(df_clean, top5_drift_features, noise_factor=0.15, shift_factor=0.25)\n",
    "df_sudden = simulate_sudden_drift(df_clean, top5_drift_features, sudden_factor=1.5, drift_point=0.5)\n",
    "df_healthy = simulate_healthy_drift(df_clean, healthy_features)\n",
    "\n",
    "# === Save Drifted Datasets ===\n",
    "\n",
    "save_dir = \"D:/August-Thesis/FL-IDS-Surveillance/data/processed/federated/unsupervised/client_3\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "df_gradual.to_csv(os.path.join(save_dir, \"train_gradual_drift.csv\"), index=False)\n",
    "df_sudden.to_csv(os.path.join(save_dir, \"train_sudden_drift.csv\"), index=False)\n",
    "df_healthy.to_csv(os.path.join(save_dir, \"train_healthy_drift.csv\"), index=False)\n",
    "\n",
    "print(\"Drifted datasets saved:\")\n",
    "print(f\"  Gradual Drift: {top5_drift_features}\")\n",
    "print(f\"  Sudden Drift: {top5_drift_features}\")\n",
    "print(f\"  Healthy Drift: {healthy_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995c044-0b1e-4460-9dbd-85ff95a94d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AR-iForest)",
   "language": "python",
   "name": "ar_iforest_fl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
